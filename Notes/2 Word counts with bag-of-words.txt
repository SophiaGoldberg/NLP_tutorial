-------------------
NLP DataCamp course
-------------------

2. Simple topic identification 
------------------------------





VIDEO 2.1. Word counts with bag-of-words 
----------------------------------------
----------------------------------------


Bag-of-words
------------
- Method for finding topics in text. 
- Create tokens & count tokens
- The more frequent a word, the more important it may be!

e.g.
----
- "The cat is in the box. The cat likes the box. The box is over the cat."
- Bag of words (stripped punctuation):
	- MOST COMMON "The": 3, "box": 3, "cat": 3, "the": 3 | NOTE that the is counted twice because it is not lower cased!
	- LESS COMMON "is": 2
	- LEAST COMMON "in": 1, "likes": 1, "over": 1

Counter/bag-of-words in Python
------------------------------

In [1]: from nltk.tokenize import word_tokenize
In [2]: from collections import Counter
In [3]: Counter(word_tokenize(
                """The cat is in the box. The cat likes the box. 
                 The box is over the cat."""))
Out[3]: 
Counter({'.': 3,
         'The': 3,
         'box': 3,
         'cat': 3,
         'in': 1,
         ...
         'the': 3})
In [4]: counter.most_common(2) # top two tokens (token, frequency) - picks two at random
Out[4]: [('The', 3), ('box', 3)]

----------------------------
CODE 2.1 Bag-of-words picker
----------------------------




VIDEO 2.2. Simple text preprocessing
------------------------------------
------------------------------------


Why preprocess?
---------------
- Better input data for ML/DS/stats
- e.g.
	- lowercasing
	- tokenization to create BOW
- Lemmatization/stemming: shorten words to root stems
- Removing stop words (e.g. and, the), punctuation, unwanted tokens


Preprocessing example
---------------------
- INPUT: 'Cats, dogs and birds are common pets. So are fish.'
- OUTPUT: cat, dog, bird, common, pet, fish (tokenized, lowercase, stop words removed, and plurals removed)


Preprocessing with Python
-------------------------

In [1]: from ntlk.corpus import stopwords #this does not run in python for some reason, would need to debug

In [2]: text = """The cat is in the box. The cat likes the box. 
                  The box is over the cat."""
In [3]: tokens = [w for w in word_tokenize(text.lower()) 
                  if w.isalpha()] #.isalpha() returns alphabetic letters if true
In [4]: no_stops = [t for t in tokens 
                    if t not in stopwords.words('english')]
In [5]: Counter(no_stops).most_common(2)
Out[5]: [('cat', 3), ('box', 3)] 
#THIS IS MUCH BETTER THAN RETURNING the AND box, as the algorithm did previously without preprocessing

-------------------------------------------------------------
CODE/TEST 2.2.1 Text preprocessing steps

Q Which of the following are useful text preprocessing steps?
A Lemmatization, lowercasing, removing unwanted tokens
-------------------------------------------------------------

--------------------------------------
CODE 2.2.2 Text preprocessing practice
--------------------------------------



2.3 Introduction to gen sim
---------------------------
---------------------------


What is gensim?
---------------

- popular NLP lib
- uses top academic models to perform complex tasks
- building document/word vectors 
- for topic idenification/document comparison

Word vecotrs
------------

- e.g. man-> woman, king-> queen
- how near/far are these words?
- SPARSE vecotrs, many zeros & few ones

Gensim ex
---------

- Laten Derichle Allocation - statistical model for topic analysis and modelling LDA
- see this link for an LDA example: http://tlfvincent.github.io/2015/10/23/presidential-speech-topics

Gensim dictionary
-----------------

ALLOWS YOU TO BUILD CORPORA (PLURAL OF CORPUS) AND DICTIONARIES

- Corpora: set of text used to help perform NLP tasks
- Dictionary() class allows us to look at tokens and ids!
- e.g...

In [1]: from gensim.corpora.dictionary import Dictionary
In [2]: from nltk.tokenize import word_tokenize
In [3]: my_documents = ['The movie was about a spaceship and aliens.',
   ...:                 'I really liked the movie!',
   ...:                 'Awesome action scenes, but boring characters.',
   ...:                 'The movie was awful! I hate alien films.',
   ...:                 'Space is cool! I liked the movie.',
   ...:                 'More space films, please!',]
In [4]: tokenized_docs = [word_tokenize(doc.lower()) 
   ...:                   for doc in my_documents]
In [5]: dictionary = Dictionary(tokenized_docs)
In [6]: dictionary.token2id
Out[6]: 
{'!': 11,
 ',': 17,
 '.': 7,
 'a': 2,
 'about': 4,
...
}

#dictionary labels each word in ALL documents with a number/token-id
#corpus is all docs and their word count/freq:
#[[doc0], ... , [(doci's use of word's token-id in the DICTIONARY , doci's count of that particular word)] ,... ,[docn]].
#creating a corpus from dictionary above

In [7]: corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]
In [8]: corpus
Out[8]: 
[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],
 [(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)],
...
] 

#(token id from dict, token frequency in doc)
#new bow model and corpus VERY QUICKLY thanks to GENSIM

- GENSIM models can be easily saved, updated and reused thanks to extra tools
- Dictionary can be updated!
- Gensim=> more advanced and feature rich BOW which can be resued in future exercies

---------------------------------------
TEST/CODE 2.3.1 What are word vectors?

Q What are word vectors and how do they help with NLP?
A Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.
---------------------------------------

-----------------------------------------------------
CODE 2.3.2 Creating and querying a corpus with gensim
-----------------------------------------------------

------------------------------
CODE 2.3.3 Gensim bag-of-words
------------------------------



VIDEO 2.4 Tf-idf with gensim
----------------------------
----------------------------

Tf-idf
------

- Term frequency - inverse document frequency
- Tf-idf => what are the important words in the document
- Each corpus may have shared words beyond just stopwords 
	- e.g. two documents talking about politics alot will not upweight 'politics', as that does not differentiate documents well
- COMMON WORDS BETWEEN DOCS should be DOWN-WEIGHTED/REMOVED

w_{i,j} = t f_{i,j} * log(N/df_i) 
WHERE:
w_{i,j} = tf-idf weight for token i in doc j
t f_{i,j} = freq occurences of token i in doc j
df_i = number of docs that contain token i
N= total number of docs

Tf-idf with gensim
------------------

In [10]: from gensim.models.tfidfmodel import TfidfModel
In [11]: tfidf = TfidfModel(corpus)
In [12]: tfidf[corpus[1]]
Out[12]: 
[(0, 0.1746298276735174),
 (1, 0.1746298276735174),
 (9, 0.29853166221463673),
 (10, 0.7716931521027908),
...
]

Tf-idf helps good topics/key-words with a corpus of shared vocabulary!

--------------------------------------------------------
TEST/CODE 2.4.1 What is tf-idf?
Q. what is tf-idf weight for token i in doc j for doc j:
tf_{i,j} = 5/100
N=200
df_i=20
A. w_{i,j} = (5 / 100) * log(200 / 20)
--------------------------------------------------------

--------------------------------
CODE 2.4.2 Tf-idf with Wikipedia
--------------------------------































































